{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Models form scratch in PyTorch\n",
    "\n",
    "No one have probably missed the recent boom and hype around Diffusion Models and their amazing results. Creating the most unimaginable images when for example given a text description. And models that top the previous best model seem to arrive every other week. Some of the best are [DALL-E 2](https://openai.com/dall-e-2/), [Imagen](https://imagen.research.google/) and a couple of weeks ago Stability AI open sourced the powerful [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release) model whom anyone can download and run. It is also fast enough to run locally on your own machine if you have a decent GPU. [Here](https://www.assemblyai.com/blog/how-to-run-stable-diffusion-locally-to-generate-images/) are instructions on how to do it, and if you have a M1 Mac there is even an [optimized app for that](https://diffusionbee.com).\n",
    "\n",
    "Now I have to admit that I have just barely scratched the surface of diffusion models and their details, I blame it on the impossible task of keeping up with high pace in the current machine learning community. So this post is about getting myself end hopefully others up to speed by implementing a diffusion model in PyTorch, train it on a dataset and touch on the details that make these models work. It will not be a dive to the depth of the ocean, in those cases I will link to good resources that I have found useful for a deep understanding of the math and derivations behind the equations. I will mix theory and implementation as I go along and the model trained will only generate images without being conditioned on any other modality, such as text, to make training feasible.\n",
    "\n",
    "### Intro and High-level Intuition\n",
    "So the basic idea of diffusion models is to learn a model that can create samples from some distribution, for example natural images, from just noise. An analogy I like to have in mind is that the models are similar to a human forming a lump of clay into for example a cup, we start with something containing no information and slowly form something useful. And we do this by our own approximation of the distribution of how a cup can be made from the lump of clay. There are infinite many ways we can go from lump of clay to cup and we can't learn everyone, but we don't have to, we only need to learn a subset that are \"good enough\". This is the core of diffusion models on a high level.\n",
    "\n",
    "### Two Phases\n",
    "The setup when training a diffusion model have two phases, a forward diffusion process, in which we over a set of timesteps $0:T$ add noise to a sample $\\mathbf{x}_0$ so we end up with the sample at time $T$ only consisting of isotropic (uniform in all dimensions) gaussian noise with mean zero and variance one. Then we have a backward diffusion process where we do the opposite, go from the noise $\\mathbf{x}_T$ to the initial sample $\\mathbf{x}_0$. It is this backward process we want to learn. The forward process is usually fixed and is just consisting of a noise scheduler which tells how much noise to add at each timestep. Lets start with digging into the forward process and take it from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Diffusion Process\n",
    "\n",
    "The noise is added by a variance schedule, $\\{\\beta\\in(0,1)\\}_{t=1}^{T}$, we will use a linear one. Lets create a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def linear_beta_schedule(timesteps: int, start: float = 0.0001, end: float = 0.02):\n",
    "    \"\"\"Creates a linear noise schedule\"\"\"\n",
    "    return torch.linspace(start, end, timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding gaussian noise from one time step to the next is then defined by\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}\\left(\\mathbf{x}_t ; \\sqrt{1 - \\beta_t}\\mathbf{x}_{t-1} , \\beta_t\\mathbf{I}\\right)\n",
    "$$\n",
    "\n",
    "And the complete forward process can be written as\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0}) = \\prod_{t=1}^{T}q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})\n",
    "$$\n",
    "\n",
    "One can see that as $t$ inscrease we go towards a normal distribution with zero mean and unit variance. If we then define a variable $\\alpha_t \\doteq 1 - \\beta_t$ and use the fact that sampling from any normal distribution can be written as the mean plus the variance times $\\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})$ (also known as the reparameterization trick) one can show that\n",
    "\n",
    "$$\n",
    "\\mathbf{x_t} = \\sqrt{\\bar{\\alpha_{t}}} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha_{t}}}\\epsilon\n",
    "\\quad \\Rightarrow \\quad\n",
    "q(\\mathbf{x}_t \\vert \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t;\\sqrt{\\bar{\\alpha_{t}}} \\mathbf{x}_0 , (1 - \\bar{\\alpha_{t}})\\mathbf{I})\n",
    "$$\n",
    "\n",
    "Where $\\bar{\\alpha_t} = \\prod_{i=0}^{t}\\alpha_i$. So we see that we can sample $\\mathbf{x_t}$ at any time $t$ given just $\\mathbf{x}_0$, i.e we can sample in closed form. This because a sums of gaussians is also gaussian. See [Lilian's excellent post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) for derivation and details.\n",
    "\n",
    "Our forward diffusion process can then be implemented as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Number of time steps in the diffusion process\n",
    "T = 300\n",
    "\n",
    "# Create the betas\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculations\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "\n",
    "def get_val_from_t(vals, t, x_shape):\n",
    "    \"\"\"\n",
    "    Helper to get specific t's of a passed list of values 'vals' and returning them with the same dimensions as 'x_shape'\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    \n",
    "    # reshape and add correct dimension and move to same device as 't'\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion(x_0, t, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Takes an initial image 'x_0' and a time step 't' and returns the \n",
    "    noisy version of the image at time 't' and the noise applied\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0).to(device)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = get_val_from_t(sqrt_alphas_cumprod, t, x_0.shape).to(device)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_val_from_t(sqrt_one_minus_alphas_cumprod, t, x_0.shape).to(device)\n",
    "    \n",
    "    return (\n",
    "        sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise,\n",
    "        noise\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In order to test the forward diffusion we need some data. We are going to use the StanfordCars dataset and for our purpose we only need to know it is a lot of images of cars, around 8000 in the training set. Let us download it ( if you running this on your own machine keep in mind it is $\\sim$ 1GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "dataset = torchvision.datasets.StanfordCars(root='data', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feeling of the images we plot some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images(dataset, num_samples=16, cols=8):\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    for i, img in enumerate(dataset):\n",
    "        if i == num_samples:\n",
    "            break\n",
    "        plt.subplot(num_samples // cols + 1, cols, i + 1)\n",
    "        plt.imshow(img[0])\n",
    "\n",
    "plot_images(dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform the data\n",
    "\n",
    "The images are in PIL format and we need them as torch tensors, this can be done using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms \n",
    "\n",
    "# resolution to use\n",
    "IMG_SIZE = 32\n",
    "\n",
    "dataset_transformed = torchvision.datasets.StanfordCars(root='data', transform=transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    # convert values to be between [-1.0,  1.0]\n",
    "    transforms.Lambda(lambda x: (x * 2) - 1)\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the image to a more resonable resolution (high resolution is very memory expensive), we also perform random horizontal flip in order to make our model more generic when it comes to orientation. Lastly we convert the image to a tensor and scale the pixel values to be between -1.0 and 1.0 since we are working with gaussians. \n",
    "\n",
    "Now we are ready to test our forward diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tensor_to_pil(image):\n",
    "    \"\"\"Reverse the transformation\"\"\"\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        # back to [0, 1] for the values\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), \n",
    "        # convert to range [0, 255]\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    return reverse_transforms(image)\n",
    "\n",
    "# take an image\n",
    "image_0 = dataset_transformed[0][0]\n",
    "\n",
    "# since plotting 'T' number of images is a bit much we decrease it\n",
    "num_images = 10\n",
    "stepsize = T // (num_images - 1)\n",
    "\n",
    "# plot the image for different t's\n",
    "fig, ax = plt.subplots(1, num_images, figsize=(20, 20), constrained_layout=True)\n",
    "for ind in range(0, num_images):\n",
    "\n",
    "    t = stepsize * ind\n",
    "    tt = torch.Tensor([t]).type(torch.int64)\n",
    "\n",
    "    image_t, _ = forward_diffusion(image_0, tt)\n",
    "    ax[ind].axis(\"off\")\n",
    "    ax[ind].set_title(f\"t = {t}\")\n",
    "    ax[ind].imshow(tensor_to_pil(image_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work quite well. Just to get indication that the last image $\\mathbf{x}_T$ indeed is similar to noise drawn from $\\sim \\mathcal{N}(0, \\mathbf{I})$ we can check to mean and standard deviation across the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std, mean = torch.std_mean(image_t)\n",
    "print(f\"Mean: {mean:.2f} Std: {std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite close, you an try and change $T$ and as you increase it we will get closer to the wanted distribution $\\mathcal{N}(0, \\mathbf{I})$. Note here that a more rigorous way would be to sample a batch of images and check mean and standard deviation across all dimensions, but lets be satisfied and continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse diffusion process\n",
    "\n",
    "So how do we go back? From noise to the original image? It might at first feel trivial since it was so easy to arrive at the noisy version. But remember, at each step when adding noise we lose a lot of information in a statistical fashion. In other words, we create an extremely large amount of possible ways to go back. In fact, to reverse the process and get back the original distribution we would have to integrate (marginalize) over all possible ways we could arrive at the original image $\\mathbf{x_0}$ including all the latent (non observed) variables on the way. This is understandably intractable in the case of natural images. So the only way we could manage this is by approximation!\n",
    "\n",
    "Here comes neural networks, once again, to the rescue. So, the idea is to train a neural network parameterized by $\\theta$ to approximate the true distribution\n",
    "\n",
    "$$ q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_{t}) \\approx p_{\\theta}(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_{t})$$\n",
    "\n",
    "The first obvious way to do this would be to estimate the mean and covariance for each denoising step.\n",
    "\n",
    "$$p_{\\theta}(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mu_{\\theta}(\\mathbf{x}_t, t), \\Sigma_{\\theta}(\\mathbf{x}_t, t))$$\n",
    "\n",
    "However the authors of the [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239v2.pdf) paper explored an alterative parameterization. Instead of estimating $ \\mu_{\\theta}(\\mathbf{x}_t, t)$ we can make the network predict the noise added at each time step, $\\epsilon_{\\theta}(\\mathbf{x}_t, t)$. This since it can be shown that (jump into the [paper]((https://arxiv.org/pdf/2006.11239v2.pdf)) for the derivation)\n",
    "\n",
    "$$ \n",
    "\\mu_{\\theta}(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha_t}}} \\epsilon_{\\theta}(\\mathbf{x}_t, t) \\right)\n",
    "$$\n",
    "\n",
    "This makes it possible to simplify the variational lower bound while maintaining performance, sounds a bit scary but let's leave it there for now.\n",
    "\n",
    "Further more they fix $\\Sigma_{\\theta}(\\mathbf{x}_t, t)$ and set it to \n",
    "\n",
    "$$\n",
    "\\Sigma_{\\theta}(\\mathbf{x}_t, t) = \\sigma_t^2\\mathbf{I} = \\tilde{\\beta_t}\\mathbf{I} = \\frac{1 - \\bar{\\alpha}_{t-1}}{1- \\bar{\\alpha}_t}\\beta_t\\mathbf{I}\n",
    "$$\n",
    "\n",
    "The reason for this is because it can be shown that $q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_{t})$ is tractable when conditioned on $\\mathbf{x}_0$ and\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_{t},\\mathbf{x}_{0}) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_{\\theta}(\\mathbf{x}_t, t), \\tilde{\\beta}_t\\mathbf{I})\n",
    "$$\n",
    "\n",
    "So when the network have predicted the noise added by one forward step we get the real image by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{x}_{t-1} \n",
    "&= \\mu_{\\theta}(\\mathbf{x}_t, t) + \\sqrt{\\tilde{\\beta}_t}\\epsilon \\\\\n",
    "&= \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha_t}}} \\epsilon_{\\theta}(\\mathbf{x}_t, t) \\right) + \\sqrt{\\tilde{\\beta}_t}\\epsilon\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can now implement a function for sampling from our model that we later will use during training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-calculate some variables\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "posterior_variance = (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod) * betas\n",
    "sqrt_one_over_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_timestep(x, t, model, device):\n",
    "    \"\"\"\n",
    "    Samples images from 'model' given image 'x' and time 't'\n",
    "    \"\"\"\n",
    "\n",
    "    betas_t = get_val_from_t(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_val_from_t(sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
    "    sqrt_one_over_alphas_t = get_val_from_t(sqrt_one_over_alphas, t, x.shape)\n",
    "    \n",
    "    posterior_mean_t = sqrt_one_over_alphas_t * (x - betas_t / sqrt_one_minus_alphas_cumprod_t * model(x, t))\n",
    "    posterior_variance_t = get_val_from_t(posterior_variance, t, x.shape)\n",
    "    \n",
    "    # do not add noise if we are at t = 0\n",
    "    noise = torch.randn_like(x) if t > 0 else 0.0\n",
    "\n",
    "    return posterior_mean_t + torch.sqrt(posterior_variance_t) * noise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also create a helper function so we can sample a number of images from our model so we later can log a bunch images for the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_images(model, device, num_samples = 5, images_per_sample = 10):\n",
    "    \"\"\"\n",
    "    Helper function to sample 'num_samples' images from the model \n",
    "    and 'images_per_sample' images for each sample spread out between t = [0, T]\n",
    "    \"\"\"\n",
    "\n",
    "    # tensor to store the results\n",
    "    images = torch.empty((num_samples, num_images, 1, 3, IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    stepsize = int(T/num_images)\n",
    "    for sample in range(num_samples):\n",
    "\n",
    "        # sample noise\n",
    "        img = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n",
    "\n",
    "        # loop backwards\n",
    "        for i in range(0, T)[::-1]:\n",
    "\n",
    "            t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "            img = sample_timestep(img, t, model, device=device)\n",
    "\n",
    "            if i % stepsize == 0:\n",
    "                # store sample image\n",
    "                col = num_images - i // stepsize - 1\n",
    "                images[sample, col] = img.detach().cpu()\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training objective\n",
    "\n",
    "So now there is only one thing missing, how do we know that our network's approximation of $q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_{t})$ is actually good? What metric can we use to measure it? The answer to this lies within a domain called [*Variational Inference*](https://en.wikipedia.org/wiki/Variational_Bayesian_methods) where the core is to say (infer) something about some unobserved (latent) variable by optimizing for a function (in our case $p_{\\theta}(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_{t})$ ). More over we are going to use something called Varational Lower Bound (VLB) or Evidence Lower Bound (ELBO), the derivation is a bit cumbersome, but lets try and get the gist of it. We are ging to start from the [*Kullback-Leibler Divergence*](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) which is a measure of how similar two distrubtions are\n",
    "\n",
    "$$\\mathbf{D}_{KL}(P \\parallel Q) = \\sum_{x\\in\\mathcal{X}}P(x)\\log\\left(\\frac{P(x)}{Q(x)}\\right)$$\n",
    "\n",
    "Lets plug in our distributions, switch to the continuous realm and include the complete sequence\n",
    "\n",
    "$$\\mathbf{D}_{KL}(q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0}) \\parallel p_{\\theta}(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})) = \\int q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})\\log\\left(\\frac{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}{p_{\\theta}(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\right) d\\mathbf{x}_{1:T}$$\n",
    "\n",
    "Here we say that given the initial image $\\mathbf{x}_0$ we want the joint probability of the latent variables $\\mathbf{x}_{1:T}$ to be the same for both distributions, which is a property we want. Lets work with this expression a bit more\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{D}_{KL}(q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0}) \\parallel p_{\\theta}(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0}))\n",
    "&=\\int q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})\\log\\left(\\frac{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}{p_{\\theta}(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\right) d\\mathbf{x}_{1:T} \\\\\n",
    "&= \\int q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})\\log\\left(\\frac{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0}) p_{\\theta}(\\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_{0:T})}\\right) d\\mathbf{x}_{1:T} & \\scriptsize{\\text{Use that } p(a \\vert b) = p(a, b)p(b)} \\\\\n",
    "&=\\mathbb{E}_{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\Big[ \\log\\left(\\frac{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0}) p_{\\theta}(\\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_{0:T})}\\right) \\Big] & \\scriptsize{\\text{From the definition of expectation}} \\\\\n",
    "&=\\mathbb{E}_{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\Big[ \\log\\left(\\frac{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}{p_{\\theta}(\\mathbf{x}_{0:T})}\\right) \\Big] + \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\Big[\\log{p_{\\theta}(\\mathbf{x}_0)}\\Big] & \\scriptsize{\\text{Split expectation into two}} \\\\\n",
    "&=\\mathbb{E}_{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\Big[ \\log\\left(\\frac{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}{p_{\\theta}(\\mathbf{x}_{0:T})}\\right) \\Big] + \\log{p_{\\theta}(\\mathbf{x}_0)}& \\scriptsize{\\text{The last term does not depend of q so the expectation can be removed}} \\\\\n",
    "&=-\\mathbb{E}_{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\Big[ \\log\\left(\\frac{p_{\\theta}(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\right) \\Big] + \\log{p_{\\theta}(\\mathbf{x}_0)}& \\scriptsize{\\text{Flip the logarithm}} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Look at the last term on the RHS $\\log{p_{\\theta}(\\mathbf{x}_0)}$, it is the log-likelihood of the distribution producing real images, which is exactly what we would like to maximize! The first term on the RHS is what is called the **Variational Lower Bound** defined as\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{VLB} = \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\Big[ \\log\\left(\\frac{p_{\\theta}(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\right) \\Big]\n",
    "$$\n",
    "\n",
    "Let us write the equation bit less verbose\n",
    "\n",
    "$$\n",
    "\\mathbf{D}_{KL} = \\log{p_{\\theta}} - \\mathcal{L}_{VLB}\n",
    "$$\n",
    "\n",
    "Why is it called the **Variational Lower Bound** and why is it useful. Let us reason a bit about the terms, $\\log{p_{\\theta}}$ is always $\\le0$ because the logarithm will be of values between 0 and 1. The $\\mathbf{D}_{KL}$ is always $\\ge0$ since it is a distance. So to make the equation valid $\\mathcal{L}_{VLB}$ must be $\\le0$. Add to this that in general $\\mathbf{D}_{KL}$ is $>0$ otherwise we would have a perfect approximation of the target distribution, so this makes $\\mathcal{L}_{VLB}$ have to be smaller than $\\log{p_{\\theta}}$, hence it is a lower limit or a **lower bound** of $\\log{p_{\\theta}}$. \n",
    "\n",
    "So it turns out that we can compute the lower bound and by maximize it we push up $\\log{p_{\\theta}}$ which is our goal. I really recommend watching [this video](https://www.youtube.com/watch?v=HxQ94L8n0vU) to get a deeper intuition about the **Variational Lower Bound**.\n",
    "<br/>\n",
    "<br/>\n",
    "The last derivation of how we reach the final loss function I leave out, but you can once again see [Lilian's great post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) or [Sohl-Dickstein et al., 2015](https://arxiv.org/abs/1503.03585) for even more details, but after some massaging one can conclude that the $\\mathcal{L}_{VLB}$ can be expanded into a sum of KL-divergence's and really the only term we need to care about is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{VLB,t} = D_\\text{KL}(q(\\mathbf{x}_t \\vert \\mathbf{x}_{t+1}, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_t \\vert\\mathbf{x}_{t+1})) \\text{ for }1 \\leq t \\leq T-1\n",
    "$$\n",
    "\n",
    "which can be computed in closed form since it as a comparison of two gaussians and further it can be shown that\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{VLB,t} = \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big]\n",
    "$$\n",
    "\n",
    "Yes, your vision is correct, after all this we are almost back at the classical mean squared error. In fact [Ho et al. (2020)](https://arxiv.org/pdf/2006.11239v2.pdf) found that the model even works better if we drop the weighting term, so our final training loss becomes just\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{VLB,t} = \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2\n",
    "$$\n",
    "\n",
    "We can sample this loss for different $t$ given an initial image $\\mathbf{x}_0$ and then update our network with the gradients as usual. Lets implement this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_loss(model, x_0, t, device):\n",
    "\n",
    "    # get noisy image and the noise from the forward diffusion\n",
    "    x_t, noise = forward_diffusion(x_0, t, device)\n",
    "\n",
    "    # predict the noise given noisy image and t\n",
    "    noise_pred = model(x_t, t)\n",
    "    \n",
    "    return F.mse_loss(noise, noise_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "\n",
    "We will use almost the same architecture as [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239v2.pdf) and I have taken the great implementation from [Lucidrains](https://github.com/lucidrains/denoising-diffusion-pytorch) and simplified it a bit. I won't go through the architecture since the code for implementing the network is pretty long and not really the core of this exploration. But on a very high level the architecture is a [U-Net](https://en.wikipedia.org/wiki/U-Net) with baked-in attention and positional encoding. If you want to take a look at it you can click and expand the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import math\n",
    "from tokenize import group\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from einops import reduce, rearrange\n",
    "\n",
    "\n",
    "def l2norm(t):\n",
    "    return F.normalize(t, dim = -1)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, img_channels: int, init_dim: int, time_emb_dim: int, num_res: int = 4):\n",
    "        \"\"\"Creates a UNet\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): number of images channels\n",
    "            init_dim (int): number of output channels in the first layer\n",
    "            time_emb_dim (int): time dimension size\n",
    "            num_res (int, optional): Number of resolutions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # initial conv\n",
    "        self.init_conv = nn.Conv2d(img_channels, init_dim, kernel_size=7, padding=3)\n",
    "        \n",
    "        # create list of the different dimensions\n",
    "        dims = [init_dim, *map(lambda m: init_dim * m, [2**res for res in range(0, num_res)])]\n",
    "\n",
    "        # create convenient list of tuples with input and output channels for each resolution\n",
    "        in_out_dims = list(zip(dims[:-1], dims[1:]))\n",
    "        \n",
    "        # time embedding block\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(init_dim),\n",
    "            nn.Linear(init_dim, time_emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # downsample\n",
    "        self.down_layers = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out_dims):\n",
    "            is_last = ind >= num_res - 1\n",
    "            \n",
    "            self.down_layers.append(nn.ModuleList([\n",
    "                ResNetBlock(dim_in, dim_in, time_emb_dim=time_emb_dim),\n",
    "                ResNetBlock(dim_in, dim_in, time_emb_dim=time_emb_dim),\n",
    "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding=1)\n",
    "            ]))\n",
    "        \n",
    "        # middle block\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = ResNetBlock(mid_dim, mid_dim, time_emb_dim=time_emb_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = ResNetBlock(mid_dim, mid_dim, time_emb_dim=time_emb_dim)\n",
    "        \n",
    "        # upsample\n",
    "        self.up_layers = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out_dims)):\n",
    "            is_last = ind == num_res - 1\n",
    "            \n",
    "            self.up_layers.append(nn.ModuleList([\n",
    "                ResNetBlock(dim_in + dim_out, dim_out, time_emb_dim=time_emb_dim),\n",
    "                ResNetBlock(dim_in + dim_out, dim_out, time_emb_dim=time_emb_dim),\n",
    "                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                Upsample(dim_out, dim_in) if not is_last else nn.Conv2d(dim_out, dim_in, 3, padding=1)\n",
    "            ]))\n",
    "        \n",
    "        self.final_res_block = ResNetBlock(init_dim * 2, init_dim, time_emb_dim = time_emb_dim)\n",
    "        self.final_conv = nn.Conv2d(init_dim, img_channels, 1)\n",
    "    \n",
    "    def forward(self, x, time):\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.down_layers:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "        \n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.up_layers:\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim = 1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "\n",
    "class WeightStandardizedConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/1903.10520\n",
    "    weight standardization purportedly works synergistically with group normalization\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "\n",
    "        weight = self.weight\n",
    "        mean = reduce(weight, 'o ... -> o 1 1 1', 'mean')\n",
    "        var = reduce(weight, 'o ... -> o 1 1 1', partial(torch.var, unbiased = False))\n",
    "        normalized_weight = (weight - mean) * (var + eps).rsqrt()\n",
    "\n",
    "        return F.conv2d(x, normalized_weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, time_emb_dim, groups = 8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, dim_out * 2)\n",
    "        )\n",
    "        \n",
    "        self.block1 = Block(dim_in, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim_in, dim_out, 1) if dim_in != dim_out else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t, time_emb = None):\n",
    "        \n",
    "        scale_shift = None\n",
    "        if self.mlp is not None and time_emb is not None:\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n",
    "            scale_shift = time_emb.chunk(2, dim = 1)\n",
    "\n",
    "        h = self.block1(x, scale_shift = scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "        \n",
    "        \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups = 8):\n",
    "        super().__init__()\n",
    "        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding = 1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift = None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if scale_shift is not None:\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "def Upsample(dim_in, dim_out):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n",
    "        nn.Conv2d(dim_in, dim_out, 3, padding = 1)\n",
    "    )\n",
    "\n",
    "def Downsample(dim_in, dim_out = None):\n",
    "    return nn.Conv2d(dim_in, dim_out, 4, 2, 1)\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n",
    "        mean = torch.mean(x, dim = 1, keepdim = True)\n",
    "        return (x - mean) * (var + eps).rsqrt() * self.g\n",
    "    \n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, dim, 1),\n",
    "            LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)\n",
    "\n",
    "        q = q.softmax(dim = -2)\n",
    "        k = k.softmax(dim = -1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        v = v / (h * w)\n",
    "\n",
    "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "\n",
    "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32, scale = 10):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)\n",
    "\n",
    "        q, k = map(l2norm, (q, k))\n",
    "\n",
    "        sim = torch.einsum('b h d i, b h d j -> b h i j', q, k) * self.scale\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = torch.einsum('b h i j, b h d j -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model and print number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(img_channels=3, init_dim=64, time_emb_dim=32)\n",
    "print(f\"Num params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34 million should be low enough to train in a reasonable amount of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "It is finally time to start training. We are going to use PyTorch Lightning which removes a lot of the boilerplate training code. In order to do this we first we need to wrap the model in a LightningModule where we define the training step and the logging of our loss and images to Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "\n",
    "        # just wrap the model\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.model(x, t)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Here we do the optimization\"\"\"\n",
    "\n",
    "        # batch is in a list\n",
    "        batch = batch[0]\n",
    "\n",
    "        batch_size = batch.shape[0]\n",
    "\n",
    "        # sample timesteps\n",
    "        t = torch.randint(0, T, (batch_size,), device=self.device).long()\n",
    "\n",
    "        # calc the loss\n",
    "        loss = get_loss(self.model, batch, t, device=self.device)\n",
    "\n",
    "        # log loss to tensorboard\n",
    "        self.log(\"Loss\", loss, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        \"\"\"Function that runs when an epoch has been trained\"\"\"\n",
    "        \n",
    "        # every 10 epoch we sample images and log to tensorboard\n",
    "        if self.trainer.current_epoch % 10 == 0:\n",
    "            images = sample_images(model=self.model, device=self.device)\n",
    "            img_h, img_w = images.shape[-2:]\n",
    "            image_grid = torchvision.utils.make_grid(images.reshape(-1, 3, img_h, img_w), nrow=images.shape[1], normalize=True)\n",
    "            self.logger.experiment.add_image('Model samples', image_grid, self.trainer.current_epoch)\n",
    "        \n",
    "        return super().on_epoch_end()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Define our optimizer\"\"\"\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.0002)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we start the training. We train for 1000 epochs with a batch size of 128 and using the Adam optimizer with a learning rate of 0.0002. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Select a subset of the data in order to converge faster\n",
    "dataset_transformed = Subset(dataset_transformed, range(0, 3000))\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset_transformed, \n",
    "    batch_size=128, \n",
    "    shuffle=True, \n",
    "    drop_last=True, \n",
    "    num_workers=24\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,              \n",
    "    precision=32,\n",
    "    max_epochs=1000,\n",
    "    logger=True,\n",
    "    log_every_n_steps=1\n",
    ")\n",
    "model = LightningModel(model)\n",
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Samples\n",
    "\n",
    "In the beginning the samples from the model is basically just noise\n",
    "\n",
    "<img src = \"images/epoch-10.png\" width=\"500\">\n",
    "\n",
    "But already at 60 epochs we start to see something that might resemble a car but still with a very abstract touch.\n",
    "\n",
    "<img src = \"images/epoch-60.png\" width=\"500\">\n",
    "\n",
    "And after training for about 2 hours and 1000 epochs the samples actually looks like cars. Still very low resolution and full with artifacts, but still decent considering the relative small model and short training time.\n",
    "\n",
    "<img src = \"images/epoch-990.png\" width=\"500\">\n",
    "\n",
    "Below is a plot of the loss. The model converge nicely and could probably have trained for quite sometime more before stagnating.\n",
    "\n",
    "<img src = \"images/loss-light.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "The math behind diffusion models is a quite complex thanks to the probabilistic nature, but in the end actually implementing one is very straightforward. This might make you ask if this whole probability framework is really needed. It might be that it is not, a recent paper [*Cold Diffusion: Inverting Arbitrary Image\n",
    "Transforms Without Noise*](https://arxiv.org/pdf/2208.09392.pdf) show that you can have completely deterministic diffusion processes and still make the models work. Sound quite promising, and simplicity usually wins long term.\n",
    "\n",
    "While the latest Text-to-Image diffusion models are remarkable, one thing I'm excited about are apply diffusion models to generate videos or Text-to-Video (T2V). And as I was finalizing this post MetaAI released there new [*Make-A-Video*](https://makeavideo.studio/). The cool thing about this model is that is not trained on text-video data but instead on text-image as usual, but then they learn how the world \"moves\" using unsupervised video and combine these to outputs to generate the videos. \n",
    "\n",
    "\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src = \"https://make-a-video.github.io/assets/T2V_samples/webp/2/Robot_dancing_in_times_square_second_upsample.webp\" width=\"300\">\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "    <small>Video generated by MetaAI's Make-A-Video model given the input <i>Robot dancing in times square</i>. <a href=\"https://make-a-video.github.io/\">source</a></small>\n",
    "</p>\n",
    "\n",
    "Sure, they are far from looking realistic, but a few papers down the road I'm sure they will start to look scary good, and the next model is most likely already getting baked in th GPUs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dl-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "340743640c1c6af8b040ece6ffa989e6aa4b04f6ae64a38402a3983a5604230f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
