{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Models form scratch in PyTorch\n",
    "\n",
    "No one have probably missed the recent boom and hype around Diffusion Models and their amazing results. Creating the most unimaginable images when for example given a text description. A couple of weeks ago Stability AI open sourced the powerful StableDiffusion model whom anyone can download and run. It is also fast enough to run locally on your own machine if you have a decent GPU.\n",
    "\n",
    "Now I have to admit that I have just barely scratched the surface of diffusion models and their details, I blame it on the impossible task of keeping up with high pace in the current machine learning community. So this post is about implementing a diffusion model in PyTorch, train it on a dataset and at least touch on the details that make these models work. It will not be a dive to the depth of ocean, in those cases I will link to good resources that I have found useful for understanding the math and the derivation behind the equations. Lets start.\n",
    "\n",
    "### Intro and High-level Intuition\n",
    "So the basic idea of diffusion models is to learn a model that can create samples from some distribution, for example natural images, from just noise. An analogy I like to have in mind is that the models are similar to a human forming a lump of clay into for example a cup, we start with something containing no information and slowly form something useful. \n",
    "\n",
    "The setup when training a diffusion model have two phases, a forward process, in which we over a set of timesteps $0:T$ add noise to a sample $\\mathbf{x}_0$ so we end up with the sample at time $T$ only consisting of isotropic (uniform in all dimensions) gaussian noise with mean zero and variance one. Then we have a backward process where we do the opposite, go from the noise $\\mathbf{x}_T$ to the initial sample $\\mathbf{x}_0$. It is this backward process we want to learn. The forward process is usually fixed and is just consisting of a noise scheduler which tells how much noise to add at each timestep. But, lets start with the forward process and take it from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Diffusion Process\n",
    "\n",
    "The noise is added by a variance schedule, $\\{\\beta\\in(0,1)\\}_{t=1}^{T}$, we will use a linear one. Lets create a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wynss/miniconda3/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def linear_beta_schedule(timesteps: int, start: float = 0.0001, end: float = 0.02):\n",
    "    \"\"\"Creates a linear noise schedule\"\"\"\n",
    "    return torch.linspace(start, end, timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding gaussian noise from one time step to the next is then defined by\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}\\left(\\mathbf{x}_t ; \\sqrt{1 - \\beta_t}\\mathbf{x}_{t-1} , \\beta_t\\mathbf{I}\\right)\n",
    "$$\n",
    "\n",
    "And the complete forward process can be written as\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0}) = \\prod_{t=1}^{T}q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})\n",
    "$$\n",
    "\n",
    "If we then define a variable $\\alpha_t \\doteq 1 - \\beta_t$ and use the fact that sampling from any normal distribution can be written as the mean plus the variance times $\\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})$ (also known as the reparameterization trick) we can show that\n",
    "\n",
    "$$\n",
    "\\mathbf{x_t} = \\sqrt{\\bar{\\alpha_{t}}} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha_{t}}}\\epsilon\n",
    "\\quad \\Rightarrow \\quad\n",
    "q(\\mathbf{x}_t \\vert \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t;\\sqrt{\\bar{\\alpha_{t}}} \\mathbf{x}_0 , (1 - \\bar{\\alpha_{t}})\\mathbf{I})\n",
    "$$\n",
    "\n",
    "Where $\\bar{\\alpha_t} = \\prod_{i=0}^{t}\\alpha_i$. So we see that we can sample $\\mathbf{x_t}$ at any time $t$ given just $\\mathbf{x}_0$, i.e we can sample in closed form. This is because a sums of gaussians is also gaussian. See [Lilian's excellent post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) for derivation and details.\n",
    "\n",
    "**Our forward diffusion process can then be implemented as**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_from_t(vals, t, x_shape):\n",
    "    \"\"\"\n",
    "    Helper to get specific t's of a passed list of values 'vals' and returning them with the same dimensions as 'x_shape'\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    \n",
    "    # reshape and add correct dimension\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion(x_0, t, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Takes an image 'x_0' and a time step 't' and returns the noisy version of the image at time 't'\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0).to(device)\n",
    "    \n",
    "    sqrt_alphas_cumprod_t = get_val_from_t(sqrt_alphas_cumprod, t, x_0.shape).to(device)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_val_from_t(sqrt_one_minus_alphas_cumprod, t, x_0.shape).to(device)\n",
    "\n",
    "    return (\n",
    "        sqrt_alphas_cumprod_t * x_0.to(device) + sqrt_one_minus_alphas_cumprod_t * noise,\n",
    "        noise\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using some variables here that we have not yet defined, these will be pre-calculated later. Otherwise that's it for the forward process! Now that we have it in place let us test it with some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We are going to use the StanfordCars dataset and for our purpose we only need to know it is a lot of images of cars. Let us download it ( if you running this on your own machine keep in mind it is $\\sim$ 1GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "dataset = torchvision.datasets.StanfordCars(root='data', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feeling of the images we plot some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(dataset, num_samples=16, cols=8):\n",
    "    \"\"\"Plot some examples\"\"\"\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    for i, img in enumerate(dataset):\n",
    "        if i == num_samples:\n",
    "            break\n",
    "        plt.subplot(num_samples // cols + 1, cols, i + 1)\n",
    "        plt.imshow(img[0])\n",
    "\n",
    "show_images(dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the data\n",
    "\n",
    "The images are in PIL format and we need them as torch tensors. Here we transform them and also put them into a DataLoader that will be used at training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms \n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "IMG_SIZE = 32\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "dataset_transformed = torchvision.datasets.StanfordCars(root='data', transform=transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    # convert values to be between [-1.0,  1.0]\n",
    "    transforms.Lambda(lambda x: (x * 2) - 1)\n",
    "]))\n",
    "\n",
    "dataset_transformed = Subset(dataset_transformed, range(0, 2000))\n",
    "\n",
    "# dimensions will be (batch, C, H, W)\n",
    "dataloader = DataLoader(dataset_transformed, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to those pre-calculated values I mentioned before. Some values will be the same for all $t$ like $\\sqrt{\\bar{\\alpha_{t}}}$ and $\\sqrt{1 - \\bar{\\alpha_{t}}}$. So instead of calculate them every time we want to get a sample $\\mathbf{x}_t$ we do it once here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Number of time steps in the diffusion process\n",
    "T = 300\n",
    "\n",
    "# Create the betas\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculations\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_one_over_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to test our forward diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tensor_to_pil(image):\n",
    "    \"\"\"Reverse the transformation and plot the image\"\"\"\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    return reverse_transforms(image)\n",
    "\n",
    "# take an image\n",
    "image_0 = next(iter(dataloader))[0][0]\n",
    "\n",
    "# since plotting 'T' number of images is a bit much we decrease it\n",
    "num_images = 10\n",
    "stepsize = T // (num_images - 1)\n",
    "\n",
    "fig, ax = plt.subplots(1, num_images, figsize=(20, 20), constrained_layout=True)\n",
    "for ind in range(0, num_images):\n",
    "\n",
    "    t = stepsize * ind\n",
    "    tt = torch.Tensor([t]).type(torch.int64)\n",
    "\n",
    "    image_t, _ = forward_diffusion(image_0, tt)\n",
    "    ax[ind].axis(\"off\")\n",
    "    ax[ind].set_title(f\"t = {t}\")\n",
    "    ax[ind].imshow(tensor_to_pil(image_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work quite well. Just to get indication that the last image $\\mathbf{x}_T$ indeed is similar to noise drawn from $\\sim \\mathcal{N}(0, \\mathbf{I})$ we can check to mean and standard deviation across the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std, mean = torch.std_mean(image_t)\n",
    "print(f\"Mean: {mean:.2f} Std: {std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite close, you an try and change $T$ and as you increase it we will get closer to the wanted distribution $\\mathcal{N}(0, \\mathbf{I})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse diffusion process\n",
    "\n",
    "So how do we go back? From noise to the original image? It might at first feel trivial since it was so easy to arrive at the noisy version. But remember, at each step when adding noise we lose information in a statistical fashion. In other words, we create an extremely large amount of possible ways to go back. In fact, to reverse the process and get back the original distribution we would have to integrate (marginalize) over all possible ways we could arrive at the original image $\\mathbf{x_0}$ including all the latent (non observed) variables on the way. This is understandably intractable in the case of natural images. So the only way we could manage this is by approximation!\n",
    "\n",
    "Here comes neural networks, once again, to the rescue. So, the idea is to train a neural network parameterized by $\\theta$ to approximate\n",
    "\n",
    "$$ q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_{t}) \\approx p_{\\theta}(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_{t})$$\n",
    "\n",
    "The first obvious way to do this would be to estimate the mean and covariance for each denoising step.\n",
    "\n",
    "$$p_{\\theta}(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mu_{\\theta}(\\mathbf{x}_t, t), \\Sigma_{\\theta}(\\mathbf{x}_t, t))$$\n",
    "\n",
    "However the authors of the [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239v2.pdf) paper explored an alterative parameterization. Instead of estimating $ \\mu_{\\theta}(\\mathbf{x}_t, t)$ we can make the network estimate the noise added at each time step, $\\epsilon_{\\theta}(\\mathbf{x}_t, t)$. This since it can be shown that (jump into the paper for the derivation)\n",
    "\n",
    "$$ \\mu_{\\theta}(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha_t}}} \\epsilon_{\\theta}(\\mathbf{x}_t, t) \\right)$$\n",
    "\n",
    "This makes it possible to simplify the variational lower bound while maintaining performance, sounds a bit scary but let's leave it there for now.\n",
    "\n",
    "### Training objective\n",
    "\n",
    "So now there is only one thing missing, how do we know that our network's approximation of $q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_{t})$ is actually good? What metric can we use to measure it? The answer to this lies within a domain called Variational Inference where the core is to say (infer) something about some unobserved (latent) variable by optimizing for a function (in our case $p_{\\theta}(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_{t})$ ). More over we are going to use something called Varational Lower Bound (VLB) or Evidence Lower Bound (ELBO), the derivation is a bit cumbersome, but lets try and get the gist of it. We are ging to start from the Kullback-Leibler Divergence which is a measure of how similar two distrubtions are.\n",
    "\n",
    "$$\\mathbf{D}_{KL}(P \\parallel Q) = \\sum_{x\\in\\mathcal{X}}P(x)\\log\\left(\\frac{P(x)}{Q(x)}\\right)$$\n",
    "\n",
    "This sounds like the perfect metric right, the similarity between two distributions, lets plug in our distributions, switch to the continuous realm and include the complete sequence\n",
    "\n",
    "$$\\mathbf{D}_{KL}(q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0}) \\parallel p_{\\theta}(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})) = \\int q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})\\log\\left(\\frac{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}{p_{\\theta}(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\right) d\\mathbf{x}_{1:T}$$\n",
    "\n",
    "Here we say that given the initial image $\\mathbf{x}_0$ we want the joint probability of the latent variables $\\mathbf{x}_{1:T}$ to be the same for both distributions, which is a property we want. Lets work with this expression a bit more\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{D}_{KL}(q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0}) \\parallel p_{\\theta}(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0}))\n",
    "&=\\int q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})\\log\\left(\\frac{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}{p_{\\theta}(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\right) d\\mathbf{x}_{1:T} \\\\\n",
    "&= \\int q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})\\log\\left(\\frac{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0}) p_{\\theta}(\\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_{0:T})}\\right) d\\mathbf{x}_{1:T} & \\scriptsize{\\text{Use that } p(a \\vert b) = p(a, b)p(b)} \\\\\n",
    "&=\\mathbb{E}_{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\Big[ \\log\\left(\\frac{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0}) p_{\\theta}(\\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_{0:T})}\\right) \\Big] & \\scriptsize{\\text{From the definition of expectation}} \\\\\n",
    "&=\\mathbb{E}_{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\Big[ \\log\\left(\\frac{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}{p_{\\theta}(\\mathbf{x}_{0:T})}\\right) \\Big] + \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\Big[\\log{p_{\\theta}(\\mathbf{x}_0)}\\Big] & \\scriptsize{\\text{Split expectation into two}} \\\\\n",
    "&=\\mathbb{E}_{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\Big[ \\log\\left(\\frac{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}{p_{\\theta}(\\mathbf{x}_{0:T})}\\right) \\Big] + \\log{p_{\\theta}(\\mathbf{x}_0)}& \\scriptsize{\\text{The last term does not depend of q so the expectation can be removed}} \\\\\n",
    "&=-\\mathbb{E}_{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\Big[ \\log\\left(\\frac{p_{\\theta}(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\right) \\Big] + \\log{p_{\\theta}(\\mathbf{x}_0)}& \\scriptsize{\\text{Flip the logarithm}} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Look at the last term on the RHS $\\log{p_{\\theta}(\\mathbf{x}_0)}$, it is the log-likelihood of the distribution producing real images, which is exactly what we would like to maximize! The first term on the RHS is what is called the **Variational Lower Bound** defined as\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{VLB} = \\mathbb{E}_{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\Big[ \\log\\left(\\frac{p_{\\theta}(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_{0})}\\right) \\Big]\n",
    "$$\n",
    "\n",
    "Let us write the equation bit less verbose\n",
    "\n",
    "$$\n",
    "\\mathbf{D}_{KL} = \\log{p_{\\theta}} - \\mathcal{L}_{VLB}\n",
    "$$\n",
    "\n",
    "But why is it called the **Variational Lower Bound** and why is it useful. Let us reason a bit about the terms, $\\log{p_{\\theta}}$ is always $\\le0$ because the logarithm will be of values between 0 and 1. The $\\mathbf{D}_{KL}$ is always $\\ge0$ since it is a distance. So to make the equation valid $\\mathcal{L}_{VLB}$ must be $\\le0$. Add to this that in general $\\mathbf{D}_{KL}$ is $>0$ otherwise we would have a perfect approximation of the target distribution, so this makes $\\mathcal{L}_{VLB}$ have to be smaller than $\\log{p_{\\theta}}$, hence it is a lower limit or a **lower bound** of $\\log{p_{\\theta}}$. So it turns out that we can compute the lower bound and by maximize it we push up $\\log{p_{\\theta}}$ which is our goal. I really recommend watching [this video](https://www.youtube.com/watch?v=HxQ94L8n0vU) to get a deeper intuition about the **Variational Lower Bound**.\n",
    "<br/>\n",
    "<br/>\n",
    "The last derivation of how we reach the final loss function I leave out, but you can once again see [Lilian's great post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/), but after some massaging one can conclude that the $\\mathcal{L}_{VLB}$ can be expanded into a sum of KL-divergence's and the only term we need to care about is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{VLB,t} = D_\\text{KL}(q(\\mathbf{x}_t \\vert \\mathbf{x}_{t+1}, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_t \\vert\\mathbf{x}_{t+1})) \\text{ for }1 \\leq t \\leq T-1\n",
    "$$\n",
    "\n",
    "which can be shown equals to\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{VLB,t} = \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big]\n",
    "$$\n",
    "\n",
    "Yes, your vision is correct, after all this we are almost back at the classical mean squared error. In fact [Ho et al. (2020)](https://arxiv.org/pdf/2006.11239v2.pdf) found that the model even works better if we drop the weighting term, so our final training objective becomes just\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}_{VLB,t}\n",
    "&= \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\\\\n",
    "&= \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha_{t}}} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha_{t}}}\\epsilon, t)\\|^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can sample this objective for different $t$ given an initial image $\\mathbf{x}_0$ and update our network with the gradients as usual. Lets implement this objective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_loss(model, x_0, t, device):\n",
    "    \"\"\"Calculate the loss for given image and t\"\"\"\n",
    "    x_t, noise = forward_diffusion(x_0, t, device)\n",
    "    noise_pred = model(x_t, t)\n",
    "    return F.l1_loss(noise, noise_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "\n",
    "We will use almost the same architecture as [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239v2.pdf) and I have taken the great implementation from [Lucidrains](https://github.com/lucidrains/denoising-diffusion-pytorch) and simplified it a bit. On a high level the architecture is a U-net with baked in attention and Resnet blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import math\n",
    "from tokenize import group\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from einops import reduce, rearrange\n",
    "\n",
    "\n",
    "def l2norm(t):\n",
    "    return F.normalize(t, dim = -1)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, img_channels: int, init_dim: int, time_emb_dim: int, num_res: int = 4):\n",
    "        \"\"\"Creates a UNet\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): number of images channels\n",
    "            init_dim (int): number of output channels in the first layer\n",
    "            time_emb_dim (int): time dimension size\n",
    "            num_res (int, optional): Number of resolutions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # initial conv\n",
    "        self.init_conv = nn.Conv2d(img_channels, init_dim, kernel_size=7, padding=3)\n",
    "        \n",
    "        # create list of the different dimensions\n",
    "        dims = [init_dim, *map(lambda m: init_dim * m, [2**res for res in range(0, num_res)])]\n",
    "\n",
    "        # create convenient list of tuples with input and output channels for each resolution\n",
    "        in_out_dims = list(zip(dims[:-1], dims[1:]))\n",
    "        \n",
    "        # time embedding block\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(init_dim),\n",
    "            nn.Linear(init_dim, time_emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # downsample\n",
    "        self.down_layers = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out_dims):\n",
    "            is_last = ind >= num_res - 1\n",
    "            \n",
    "            self.down_layers.append(nn.ModuleList([\n",
    "                ResNetBlock(dim_in, dim_in, time_emb_dim=time_emb_dim),\n",
    "                ResNetBlock(dim_in, dim_in, time_emb_dim=time_emb_dim),\n",
    "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding=1)\n",
    "            ]))\n",
    "        \n",
    "        # middle block\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = ResNetBlock(mid_dim, mid_dim, time_emb_dim=time_emb_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = ResNetBlock(mid_dim, mid_dim, time_emb_dim=time_emb_dim)\n",
    "        \n",
    "        # upsample\n",
    "        self.up_layers = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out_dims)):\n",
    "            is_last = ind == num_res - 1\n",
    "            \n",
    "            self.up_layers.append(nn.ModuleList([\n",
    "                ResNetBlock(dim_in + dim_out, dim_out, time_emb_dim=time_emb_dim),\n",
    "                ResNetBlock(dim_in + dim_out, dim_out, time_emb_dim=time_emb_dim),\n",
    "                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                Upsample(dim_out, dim_in) if not is_last else nn.Conv2d(dim_out, dim_in, 3, padding=1)\n",
    "            ]))\n",
    "        \n",
    "        self.final_res_block = ResNetBlock(init_dim * 2, init_dim, time_emb_dim = time_emb_dim)\n",
    "        self.final_conv = nn.Conv2d(init_dim, img_channels, 1)\n",
    "    \n",
    "    def forward(self, x, time):\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.down_layers:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "        \n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.up_layers:\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim = 1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "\n",
    "class WeightStandardizedConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/1903.10520\n",
    "    weight standardization purportedly works synergistically with group normalization\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "\n",
    "        weight = self.weight\n",
    "        mean = reduce(weight, 'o ... -> o 1 1 1', 'mean')\n",
    "        var = reduce(weight, 'o ... -> o 1 1 1', partial(torch.var, unbiased = False))\n",
    "        normalized_weight = (weight - mean) * (var + eps).rsqrt()\n",
    "\n",
    "        return F.conv2d(x, normalized_weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, time_emb_dim, groups = 8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, dim_out * 2)\n",
    "        )\n",
    "        \n",
    "        self.block1 = Block(dim_in, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim_in, dim_out, 1) if dim_in != dim_out else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t, time_emb = None):\n",
    "        \n",
    "        scale_shift = None\n",
    "        if self.mlp is not None and time_emb is not None:\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n",
    "            scale_shift = time_emb.chunk(2, dim = 1)\n",
    "\n",
    "        h = self.block1(x, scale_shift = scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "        \n",
    "        \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups = 8):\n",
    "        super().__init__()\n",
    "        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding = 1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift = None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if scale_shift is not None:\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "def Upsample(dim_in, dim_out):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n",
    "        nn.Conv2d(dim_in, dim_out, 3, padding = 1)\n",
    "    )\n",
    "\n",
    "def Downsample(dim_in, dim_out = None):\n",
    "    return nn.Conv2d(dim_in, dim_out, 4, 2, 1)\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n",
    "        mean = torch.mean(x, dim = 1, keepdim = True)\n",
    "        return (x - mean) * (var + eps).rsqrt() * self.g\n",
    "    \n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, dim, 1),\n",
    "            LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)\n",
    "\n",
    "        q = q.softmax(dim = -2)\n",
    "        k = k.softmax(dim = -1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        v = v / (h * w)\n",
    "\n",
    "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "\n",
    "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32, scale = 10):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)\n",
    "\n",
    "        q, k = map(l2norm, (q, k))\n",
    "\n",
    "        sim = torch.einsum('b h d i, b h d j -> b h i j', q, k) * self.scale\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = torch.einsum('b h i j, b h d j -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model and print number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(img_channels=3, init_dim=64, time_emb_dim=32)\n",
    "print(f\"Num params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34 million should be low enough to train in a reasonable amount of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will create some helper functions to sample from our model as well as plot the samples for different $t$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_timestep(x, t, model, device):\n",
    "    \"\"\"\n",
    "    Calls the model to predict the noise in the image and returns \n",
    "    the denoised image. \n",
    "    Applies noise to this image, if we are not in the last step yet.\n",
    "    \"\"\"\n",
    "    betas_t = get_val_from_t(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_val_from_t(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_one_over_alphas_t = get_val_from_t(sqrt_one_over_alphas, t, x.shape)\n",
    "    \n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_one_over_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    posterior_variance_t = get_val_from_t(posterior_variance, t, x.shape)\n",
    "    \n",
    "    if t == 0:\n",
    "        return model_mean\n",
    "    \n",
    "    noise = torch.randn_like(x)\n",
    "    return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_images(model, device, num_samples = 5, images_per_sample = 10):\n",
    "\n",
    "    images = torch.empty((num_samples, num_images, 1, 3, IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    stepsize = int(T/num_images)\n",
    "\n",
    "    for sample in range(num_samples):\n",
    "        # Sample noise\n",
    "        img = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n",
    "\n",
    "        for i in range(0, T)[::-1]:\n",
    "            t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "            img = sample_timestep(img, t, model, device=device)\n",
    "\n",
    "            if i % stepsize == 0:\n",
    "                col = num_images - i // stepsize - 1\n",
    "                images[sample, col] = img.detach().cpu()\n",
    "    \n",
    "    return images\n",
    "\n",
    "def plot_sampled_images(images):\n",
    "    num_samples, num_images, *_ = images.shape\n",
    "    fig, ax = plt.subplots(num_samples, num_images, figsize=(20, 20), constrained_layout=True)\n",
    "\n",
    "    for sample in range(num_samples):\n",
    "        for image in range(num_images):\n",
    "            ax[sample, image].axis(\"off\")\n",
    "            ax[sample, image].imshow(tensor_to_pil(images[sample, image]))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now it is finally time to start the training, so we define a simple training loop using our dataloader. We track the loss in tensorboard and store the samples from our model along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "writer = SummaryWriter()\n",
    "global_step = 0\n",
    "\n",
    "EPOCHS = 500 # Try more!\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # sample t's\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "\n",
    "        loss = get_loss(model, batch[0], t, device=device)\n",
    "        writer.add_scalar(\"Loss\", loss, global_step=global_step)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        global_step += 1\n",
    "  \n",
    "    if epoch % 5 == 0:\n",
    "      print(f\"epoch: {epoch:4} | loss {loss.detach().cpu()}\")\n",
    "      images = sample_images(model=model, device=device)\n",
    "      image_grid = make_grid(images.reshape(-1, 3, IMG_SIZE, IMG_SIZE), nrow=images.shape[1], normalize=True)\n",
    "      writer.add_image('images', image_grid, global_step=epoch)\n",
    "      #plot_sampled_images(images[0:2])\n",
    "  \n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "plt.imshow(tensor_to_pil(images[0]))\n",
    "print(image_0.shape)\n",
    "\n",
    "image_grid = make_grid(images[0:10], normalize=True)\n",
    "writer.add_image(\"test_image\", image_grid)\n",
    "# writer.add_image(\"test_image\", reverse_transforms(image_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_transforms = transforms.Compose([\n",
    "    transforms.Lambda(lambda t: (t + 1) / 2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = next(iter(dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29b625e07d616bed27f29d7e5d87cfb69a98bc2535c9b9402c264476aa37b4fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
